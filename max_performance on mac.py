from llama_cpp import Llama
import os
import time

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Metal
os.environ['METAL_DEVICE_WRITABLE_MEMORY'] = '1'
os.environ['LLAMA_METAL_PATH_OVERRIDE'] = './models'
os.environ['LLAMA_METAL_NDEBUG'] = '1'


class UltraFastAssistant:
    def __init__(self):
        print("‚ö° –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Metal Turbo...")
        self.llm = Llama(
            model_path="models/Mistral-Nemo-Instruct-2407-Q5_K_M.gguf",
            n_ctx=512,
            n_gpu_layers=-1,
            n_threads=10,  # Threads –∑–∞–¥–∞—é—Ç—Å—è –∑–¥–µ—Å—å, –Ω–µ –≤ –≤—ã–∑–æ–≤–µ
            n_batch=1024,
            f16_kv=True,
            verbose=False,
            seed=42,
            embedding=False,
            logits_all=False,
            use_mlock=True,
            main_gpu=0,
            tensor_split=[0]
        )
        print("‚úì Metal Turbo –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!")

    def get_response(self, user_input):
        prompt = f"""–£: {user_input}
–û:"""

        start = time.time()

        response = self.llm(
            prompt,
            max_tokens=128,
            temperature=0.7,
            top_p=0.9,
            top_k=40,
            repeat_penalty=1.1,
            stream=True,
            stop=["–£:", "–û:"],
            mirostat_mode=2,
            mirostat_tau=5.0,
            mirostat_eta=0.1
        )

        print("\nV: ", end="", flush=True)
        full_response = ""
        for token in response:
            chunk = token['choices'][0]['text']
            print(chunk, end="", flush=True)
            full_response += chunk

        elapsed = time.time() - start
        print(f"\n[‚ö° {elapsed:.2f}s]")
        return full_response


def main():
    assistant = UltraFastAssistant()
    print("\nüöÄ Metal Turbo –≥–æ—Ç–æ–≤! (exit –¥–ª—è –≤—ã—Ö–æ–¥–∞)")

    while True:
        user_input = input("\n–í—ã: ").strip()
        if user_input.lower() == 'exit':
            break
        if user_input:
            assistant.get_response(user_input)


if __name__ == "__main__":
    main()