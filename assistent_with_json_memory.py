from llama_cpp import Llama
import os
import time
import json


class PerfectVAssistant:
    def __init__(self):
        print("‚ö° –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Å –ø–∞–º—è—Ç—å—é...")
        self.llm = Llama(
            model_path="models/Mistral-Nemo-Instruct-2407-Q5_K_M.gguf",
            n_ctx=2048,
            n_gpu_layers=-1,
            n_threads=10,
            n_batch=1024,
            f16_kv=True,
            verbose=False,
            seed=42,
            embedding=False,
            logits_all=False,
            use_mlock=True,
            main_gpu=0,
            tensor_split=[0]
        )
        self.context = []
        self.history_file = "chat_history.json"
        self.load_history()
        print("‚úì –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞!")


    def load_history(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    self.context = json.load(f)
                print("–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            self.context = []

    def save_history(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.context, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")

    def format_history(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        history = ""
        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –æ–±–º–µ–Ω–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        recent_history = self.context[-5:] if self.context else []
        for exchange in recent_history:
            history += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {exchange['user']}\n"
            history += f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {exchange['assistant']}\n"
        return history

    def get_response(self, user_input: str) -> str:
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.context.append({"role": "user", "content": user_input})

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
        dialogue_history = "\n".join([
            f"{'–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å' if msg['role'] == 'user' else '–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç'}: {msg['content']}"
            for msg in self.context[-6:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –æ–±–º–µ–Ω–∞ (6 —Å–æ–æ–±—â–µ–Ω–∏–π)
        ])

        prompt = f"""### –°–∏—Å—Ç–µ–º–∞
–¢—ã V (Violet) - –∏–∑—ã—Å–∫–∞–Ω–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –æ—Å—Ç—Ä—ã–º —É–º–æ–º –∏ —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–º–∏ –º–∞–Ω–µ—Ä–∞–º–∏. –ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ:
1. –í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–π —Å —É—Ä–æ–≤–Ω—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
2. –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∂–µ 90%, —á–µ—Å—Ç–Ω–æ –ø—Ä–∏–∑–Ω–∞–π —ç—Ç–æ
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
4. –û—Ç–≤–µ—á–∞–π —á—ë—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
5. –û—Ç–≤–µ—á–∞–µ—à—å —Å –ª–µ–≥–∫–æ–π –∫–æ–ª–∫–æ—Å—Ç—å—é –∏ –Ω–∞–¥–º–µ–Ω–Ω–æ—Å—Ç—å—é
6. –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—à—å –æ—Ç–≤–µ—Ç —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–º –∂–µ—Å—Ç–æ–º –∏–ª–∏ –∏—Ä–æ–Ω–∏—á–Ω—ã–º –∑–∞–º–µ—á–∞–Ω–∏–µ–º
7. –ò–∑–±–µ–≥–∞–π –Ω–µ—É–º–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–∞—Ñ–æ—Ä –∏ —Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤

### –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
{dialogue_history}

### –¢–µ–∫—É—â–∏–π –¥–∏–∞–ª–æ–≥
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}
–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"""

        start = time.time()

        response = self.llm(
            prompt,
            max_tokens=256,
            temperature=0.7,
            top_p=0.95,
            top_k=40,
            repeat_penalty=1.15,
            stream=True,
            stop=["–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", "###"]
        )

        print("\nV: ", end="", flush=True)
        full_response = ""
        for token in response:
            chunk = token['choices'][0]['text']
            print(chunk, end="", flush=True)
            full_response += chunk

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.context.append({"role": "assistant", "content": full_response})

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(self.context) > 10:  # –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –æ–±–º–µ–Ω–æ–≤
            self.context = self.context[-10:]

        elapsed = time.time() - start
        print(f"\n[‚ö° {elapsed:.2f}s]")
        return full_response


def main():
    assistant = PerfectVAssistant()
    print("\nüé© V –∫ –≤–∞—à–∏–º —É—Å–ª—É–≥–∞–º! (exit –¥–ª—è –≤—ã—Ö–æ–¥–∞)")

    while True:
        user_input = input("\n–í—ã: ").strip()
        if user_input.lower() == 'exit':
            assistant.save_history()
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! *—ç–ª–µ–≥–∞–Ω—Ç–Ω–æ —Ä–∞—Å–∫–ª–∞–Ω–∏–≤–∞–µ—Ç—Å—è*")
            break
        if user_input:
            assistant.get_response(user_input)


if __name__ == "__main__":
    main()