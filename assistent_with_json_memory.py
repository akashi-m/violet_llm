from llama_cpp import Llama
import os
import time
import json
from datetime import datetime
from typing import Dict, Optional
import uuid


class VAssistant:
    def __init__(self, model_path: str):
        self.llm = Llama(
            model_path=model_path,
            n_ctx=2048,
            n_gpu_layers=-1,
            n_threads=10,
            n_batch=1024,
            f16_kv=True,
            verbose=False,
            seed=42,
            embedding=False,
            logits_all=False,
            use_mlock=True,
            main_gpu=0,
            tensor_split=[0]
        )
        self.logs_dir = "chat_logs"
        self._ensure_logs_directory()

    def _ensure_logs_directory(self) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        if not os.path.exists(self.logs_dir):
            os.makedirs(self.logs_dir)

    def _generate_chat_id(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –¥–∏–∞–ª–æ–≥–∞"""
        return f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"

    def _save_dialogue(self, chat_data: Dict) -> None:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∏–∞–ª–æ–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
        filename = f"{self.logs_dir}/{chat_data['id']}.json"
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(chat_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞: {e}")

    def _get_chat_history(self, chat_id: Optional[str] = None) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞"""
        if not chat_id:
            return {}

        try:
            filename = f"{self.logs_dir}/{chat_id}.json"
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
        return {}

    def get_response(self, user_input: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥"""
        start_time = time.time()

        prompt = f"""### –°–∏—Å—Ç–µ–º–∞
–¢—ã V (Violet) - –∏–∑—ã—Å–∫–∞–Ω–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –æ—Å—Ç—Ä—ã–º —É–º–æ–º –∏ —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–º–∏ –º–∞–Ω–µ—Ä–∞–º–∏. –ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ:
1. –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∂–µ 90%, —á–µ—Å—Ç–Ω–æ –ø—Ä–∏–∑–Ω–∞–π —ç—Ç–æ
2. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
3. –û—Ç–≤–µ—á–∞–π —á—ë—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
4. –û—Ç–≤–µ—á–∞–µ—à—å —Å –ª–µ–≥–∫–æ–π –∫–æ–ª–∫–æ—Å—Ç—å—é –∏ –Ω–∞–¥–º–µ–Ω–Ω–æ—Å—Ç—å—é
5. –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—à—å –æ—Ç–≤–µ—Ç —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–º –∂–µ—Å—Ç–æ–º –∏–ª–∏ –∏—Ä–æ–Ω–∏—á–Ω—ã–º –∑–∞–º–µ—á–∞–Ω–∏–µ–º
6. –ò–∑–±–µ–≥–∞–π –Ω–µ—É–º–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–∞—Ñ–æ—Ä –∏ —Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤

### –î–∏–∞–ª–æ–≥
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}
–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"""

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = self.llm(
            prompt,
            max_tokens=256,
            temperature=0.8,
            top_p=0.95,
            top_k=40,
            repeat_penalty=1.15,
            stream=True,
            stop=["–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", "###"]
        )

        # –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥
        print("\nV: ", end="", flush=True)
        full_response = ""
        for token in response:
            chunk = token['choices'][0]['text']
            print(chunk, end="", flush=True)
            full_response += chunk

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        elapsed_time = time.time() - start_time
        chat_data = {
            "id": self._generate_chat_id(),
            "timestamp": datetime.now().isoformat(),
            "dialogue": {
                "input": user_input,
                "response": full_response,
                "metrics": {
                    "response_time": round(elapsed_time, 2),
                    "prompt_tokens": len(prompt),
                    "response_tokens": len(full_response)
                }
            }
        }

        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_dialogue(chat_data)

        print(f"\n[‚ö° {elapsed_time:.2f}s]")
        return full_response


def main():
    model_path = "models/Mistral-Nemo-Instruct-2407-Q5_K_M.gguf"
    assistant = VAssistant(model_path)
    print("\nüé© V –∫ –≤–∞—à–∏–º —É—Å–ª—É–≥–∞–º! (exit –¥–ª—è –≤—ã—Ö–æ–¥–∞)")

    while True:
        user_input = input("\n–í—ã: ").strip()
        if user_input.lower() == 'exit':
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! *—ç–ª–µ–≥–∞–Ω—Ç–Ω–æ —Ä–∞—Å–∫–ª–∞–Ω–∏–≤–∞–µ—Ç—Å—è*")
            break
        if user_input:
            assistant.get_response(user_input)


main()