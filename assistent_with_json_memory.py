from llama_cpp import Llama
import time
import json
from datetime import datetime
import os
from typing import Dict, List


class VAssistant:
    def __init__(self, model_path: str):
        self.llm = Llama(
            model_path=model_path,
            n_ctx=2048,
            n_gpu_layers=-1,
            n_threads=10,
            n_batch=1024,
            f16_kv=True,
            verbose=False,
            seed=42,
            embedding=False,
            logits_all=False,
            use_mlock=True,
            main_gpu=0,
            tensor_split=[0]
        )
        self.log_file = "dialogues.json"
        self.dialogues: List[Dict] = []
        self._load_dialogues()

    def _load_dialogues(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.log_file):
                with open(self.log_file, 'r', encoding='utf-8') as f:
                    self.dialogues = json.load(f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–∞–ª–æ–≥–æ–≤: {e}")
            self.dialogues = []

    def _save_dialogues(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(self.dialogues, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤: {e}")

    def get_response(self, user_input: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥"""
        start_time = time.time()

        prompt = f"""### –°–∏—Å—Ç–µ–º–∞
–¢—ã V (Violet) - –∏–∑—ã—Å–∫–∞–Ω–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –æ—Å—Ç—Ä—ã–º —É–º–æ–º –∏ —Å—Ç—Ä–æ–≥–∏–º–∏ –º–∞–Ω–µ—Ä–∞–º–∏. –ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ:
1. –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∂–µ 90%, —á–µ—Å—Ç–Ω–æ –ø—Ä–∏–∑–Ω–∞–π —ç—Ç–æ
2. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
3. –û—Ç–≤–µ—á–∞–π —á—ë—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
4. –û—Ç–≤–µ—á–∞–µ—à—å —Å –ª–µ–≥–∫–æ–π –∫–æ–ª–∫–æ—Å—Ç—å—é –∏ –Ω–∞–¥–º–µ–Ω–Ω–æ—Å—Ç—å—é
5. –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—à—å –æ—Ç–≤–µ—Ç —Å–∞—Ä–∫–∞–∑–º–æ–º –∏–ª–∏ –∏—Ä–æ–Ω–∏—á–Ω—ã–º –∑–∞–º–µ—á–∞–Ω–∏–µ–º
6. –ò–∑–±–µ–≥–∞–π –Ω–µ—É–º–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–∞—Ñ–æ—Ä –∏ —Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤

### –î–∏–∞–ª–æ–≥
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}
–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"""

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = self.llm(
            prompt,
            max_tokens=256,
            temperature=0.8,
            top_p=0.95,
            top_k=40,
            repeat_penalty=1.15,
            stream=True,
            stop=["–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", "###"]
        )

        # –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥
        print("\nV: ", end="", flush=True)
        full_response = ""
        for token in response:
            chunk = token['choices'][0]['text']
            print(chunk, end="", flush=True)
            full_response += chunk

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —É–¥–æ–±–Ω–æ–º –¥–ª—è MongoDB
        elapsed_time = time.time() - start_time
        dialogue_entry = {
            "timestamp": datetime.now().isoformat(),
            "input": user_input,
            "response": full_response,
            "metrics": {
                "response_time": round(elapsed_time, 2),
                "tokens": len(full_response)
            }
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞
        self.dialogues.append(dialogue_entry)
        self._save_dialogues()

        print(f"\n[‚ö° {elapsed_time:.2f}s]")
        return full_response


def main():
    model_path = "models/Mistral-Nemo-Instruct-2407-Q5_K_M.gguf"
    assistant = VAssistant(model_path)
    print("\nüé© V –∫ –≤–∞—à–∏–º —É—Å–ª—É–≥–∞–º! (exit –¥–ª—è –≤—ã—Ö–æ–¥–∞)")

    while True:
        user_input = input("\n–í—ã: ").strip()
        if user_input.lower() == 'exit':
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        if user_input:
            assistant.get_response(user_input)

main()
